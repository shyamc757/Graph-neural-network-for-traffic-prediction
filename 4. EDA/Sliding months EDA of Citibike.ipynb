{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c51de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"EDA/Citibike/sliding_months\"\n",
    "if not (os.path.exists(base_dir)):\n",
    "              os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a018f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_graph_creation(df, window_len, overlap_len):\n",
    "    \n",
    "    df = df[(df['start_zone_encoded'] != 264) & (df['start_zone_encoded'] != 265) & (df['end_zone_encoded'] != 264) & (df['end_zone_encoded'] != 265)]\n",
    "    df = df.sort_values(by = 'start_time').reset_index()\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'], unit = 's')\n",
    "    df['stop_time'] = pd.to_datetime(df['stop_time'], unit = 's')\n",
    "    df = df[['start_zone_encoded', 'end_zone_encoded', 'user_member_encoded', 'start_time', 'stop_time', 'weight']]\n",
    "\n",
    "    window_size = pd.Timedelta(days=window_len)\n",
    "    overlap_size = pd.Timedelta(days=overlap_len)\n",
    "\n",
    "    # Adjust start time to the beginning of the day\n",
    "    start_time = df['start_time'].min().normalize()\n",
    "    # print(f'start time : {start_time}')\n",
    "\n",
    "    # Adjust stop time to the end of the day\n",
    "    stop_time = (df['stop_time'].max() + pd.Timedelta(days=1)).normalize()\n",
    "    # print(f'stop time : {stop_time}')\n",
    "\n",
    "    window_graphs_total = []\n",
    "    window_graphs_customers = []\n",
    "    window_graphs_subscribers = []\n",
    "\n",
    "    days = []\n",
    "    \n",
    "    end_time = start_time + window_size\n",
    "    # print(f'end time : {end_time}')\n",
    "    # Iterate over sliding windows with overlap\n",
    "    while end_time <= stop_time:\n",
    "        # Filter the DataFrame for data within the current window\n",
    "        window_data = df[(df['start_time'] >= start_time) & (df['stop_time'] < end_time)]\n",
    "\n",
    "        total_df = window_data.groupby(['start_zone_encoded', 'end_zone_encoded']).agg({'weight':'sum'}).reset_index()\n",
    "        customers_df = window_data.groupby(['start_zone_encoded', 'end_zone_encoded', 'user_member_encoded']).agg({'weight':'sum'}).reset_index()\n",
    "        subscribers_df = window_data.groupby(['start_zone_encoded', 'end_zone_encoded', 'user_member_encoded']).agg({'weight':'sum'}).reset_index()\n",
    "\n",
    "        customers_df = customers_df[customers_df['user_member_encoded'] == 1]\n",
    "        subscribers_df = subscribers_df[subscribers_df['user_member_encoded'] == 0]\n",
    "\n",
    "#         total_df.rename(columns = {'stop_time' : 'weight'}, inplace = True)\n",
    "#         customers_df.rename(columns = {'stop_time' : 'weight'}, inplace = True)\n",
    "#         subscribers_df.rename(columns = {'stop_time' : 'weight'}, inplace = True)\n",
    "\n",
    "        G_total = nx.from_pandas_edgelist(total_df, source='start_zone_encoded', target='end_zone_encoded', edge_attr='weight', create_using=nx.DiGraph)\n",
    "        G_customers = nx.from_pandas_edgelist(customers_df, source='start_zone_encoded', target='end_zone_encoded', edge_attr='weight', create_using=nx.DiGraph)\n",
    "        G_subscribers = nx.from_pandas_edgelist(subscribers_df, source='start_zone_encoded', target='end_zone_encoded', edge_attr='weight', create_using=nx.DiGraph)\n",
    "\n",
    "        G_total.remove_edges_from(nx.selfloop_edges(G_total))\n",
    "        G_customers.remove_edges_from(nx.selfloop_edges(G_customers))\n",
    "        G_subscribers.remove_edges_from(nx.selfloop_edges(G_subscribers))\n",
    "\n",
    "        window_graphs_total.append(G_total)\n",
    "        window_graphs_customers.append(G_customers)\n",
    "        window_graphs_subscribers.append(G_subscribers)\n",
    "\n",
    "        days.append((start_time.strftime('%m-%d'),end_time.strftime('%m-%d\\'%y')))\n",
    "\n",
    "        # Move to the next window with overlap\n",
    "        start_time += overlap_size\n",
    "        end_time += overlap_size\n",
    "        \n",
    "    return window_graphs_total, window_graphs_customers, window_graphs_subscribers, days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c24321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_EDA(graphs, window, string):\n",
    "    \n",
    "    base_eda_dir = f\"{base_dir}/{string}\"\n",
    "# =========================================================================================================================\n",
    "    # number of edges\n",
    "    num_nodes = [current_graph.number_of_nodes() for current_graph in graphs]\n",
    "    num_arcs = [current_graph.number_of_edges() for current_graph in graphs]\n",
    "\n",
    "    df1 = pd.DataFrame({f'n_arcs_{string}':num_arcs}, index=window)\n",
    "    df1.plot(figsize=(12, 6), style={f'n_arcs_{string}': '-'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('count')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title1 = \"\"\n",
    "    if string == 'total':  \n",
    "#         plt.title('Evolution of no. of citibike trips')\n",
    "        title1 = \"Evolution of no. of edges in citibike trips network\"\n",
    "    else:\n",
    "#         plt.title(f'Evolution of no. of citibike trips by {string}s')\n",
    "        title1 = f\"Evolution of no. of edges in citibike trips network of {string}s\"\n",
    "    plt.title(title1)\n",
    "    plt.grid()\n",
    "    if string == 'total':  \n",
    "        plt.legend(['Total edges'])\n",
    "    else:\n",
    "        plt.legend([f'Total edges by {string}s'])\n",
    "    \n",
    "    temp1 = title1+\".png\"\n",
    "    full_path1 = os.path.join(base_eda_dir,temp1)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path1, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# ======================================================================================================================\n",
    "#     number of trips\n",
    "    \n",
    "    total_weight_total = [sum(G.edges[edge]['weight'] for edge in G.edges()) for G in graphs]\n",
    "\n",
    "    df_new = pd.DataFrame({f'n_trips_{string}':total_weight_total}, index=window)\n",
    "    df_new.plot(figsize=(12, 6), style={f'n_trips_{string}': '-'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('count')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title_new = \"\"\n",
    "    if string == 'total':  \n",
    "#         plt.title('Evolution of no. of citibike trips')\n",
    "        title_new = \"Evolution of no. of citibike trips\"\n",
    "    else:\n",
    "#         plt.title(f'Evolution of no. of citibike trips by {string}s')\n",
    "        title_new = f\"Evolution of no. of citibike trips by {string}s\"\n",
    "    plt.title(title_new)\n",
    "    plt.grid()\n",
    "    if string == 'total':  \n",
    "        plt.legend(['Total trips'])\n",
    "    else:\n",
    "        plt.legend([f'Total trips by {string}s'])\n",
    "    \n",
    "    temp_new = title_new+\".png\"\n",
    "    full_path_new = os.path.join(base_eda_dir,temp_new)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path_new, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# =================================================================================================================\n",
    "#     number of nodes\n",
    "    \n",
    "    df2 = pd.DataFrame({f'n_nodes_{string}':num_nodes}, index=window)\n",
    "    df2.plot(figsize=(12, 6), style={f'n_nodes_{string}': '-'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('count')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title2 = \"\"\n",
    "    if string == 'total':  \n",
    "#         plt.title('Evolution of no. of citibike stations used')\n",
    "        title2 = \"Evolution of no. of citibike stations used\"\n",
    "    else:\n",
    "#         plt.title(f'Evolution of no. of citibike stations used by {string}s')\n",
    "        title2 = f\"Evolution of no. of citibike stations used by {string}s\"\n",
    "    plt.title(title2)\n",
    "    plt.grid()\n",
    "    if string == 'total':  \n",
    "        plt.legend(['No. of stations used'])\n",
    "    else:\n",
    "        plt.legend([f'No. of stations used by {string}s'])\n",
    "        \n",
    "    temp2 = title2+\".png\"\n",
    "    full_path2 = os.path.join(base_eda_dir,temp2)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path2, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# ===========================================================================================================================\n",
    "    # Average degree\n",
    "    \n",
    "    avg_degree = [sum(dict(G.in_degree()).values()) / len(G) if len(G) > 0 else 0 for G in graphs]\n",
    "    \n",
    "    df3 = pd.DataFrame({f'avg_degree_{string}':avg_degree}, index=window)\n",
    "    df3.plot(figsize=(12, 6), style={f'avg_degree_{string}': '-'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('Average Degree')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title3 = \"\"\n",
    "    if string == 'total':  \n",
    "        title3 = \"Evolution of Average Degree of citibike trips\"\n",
    "#         plt.title('Evolution of Average Degree of citibike trips')\n",
    "    else:\n",
    "        title3 = f\"Evolution of Average Degree of citibike trips by {string}s\"\n",
    "#         plt.title(f'Evolution of Average Degree of citibike trips by {string}s')\n",
    "    plt.title(title3)\n",
    "    plt.grid()\n",
    "    plt.legend(['Average degree'])\n",
    "    temp3 = title3+\".png\"\n",
    "    full_path3 = os.path.join(base_eda_dir,temp3)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path3, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# =======================================================================================================================\n",
    "    # Clustering Coefficient\n",
    "    clustering_coeff = [nx.average_clustering(G) if len(G) > 0 else 0 for G in graphs]\n",
    "\n",
    "    df4 = pd.DataFrame({f'clustering_coeff_{string}':clustering_coeff}, index=window)\n",
    "    df4.plot(figsize=(12, 6), style={f'clustering_coeff_{string}': '-'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('Clustering Coefficient')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title4 = \"\"\n",
    "    if string == 'total':  \n",
    "        title4 = 'Evolution of clustering coefficient of citibike trips'\n",
    "#         plt.title('Evolution of clustering coefficient of citibike trips')\n",
    "    else:\n",
    "        title4 = f'Evolution of clustering coefficient of citibike trips of {string}s'\n",
    "#         plt.title(f'Evolution of clustering coefficient of citibike trips of {string}s')\n",
    "    plt.title(title4)\n",
    "    plt.grid()\n",
    "    plt.legend(['Average Clustering Coefficient'])\n",
    "    temp4 = title4+\".png\"\n",
    "    full_path4 = os.path.join(base_eda_dir,temp4)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path4, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# ===========================================================================================================================  \n",
    "    # Community detection and its similarity\n",
    "    \n",
    "    temp_module = nx.algorithms.community.louvain\n",
    "    list_of_partition_labels = [[0]*264 for _ in range(len(graphs))]\n",
    "\n",
    "    count = 0\n",
    "    for G in graphs:\n",
    "\n",
    "        if len(G) > 0:\n",
    "            partitions = temp_module.louvain_communities(G, weight = 'weight', seed = 42)\n",
    "        else:\n",
    "            count+=1\n",
    "            continue\n",
    "\n",
    "        for i in range(len(partitions)):\n",
    "            for item in partitions[i]:\n",
    "                list_of_partition_labels[count][item] = i+1\n",
    "\n",
    "        count+=1\n",
    "    \n",
    "    from sklearn.metrics import adjusted_mutual_info_score as AMI, normalized_mutual_info_score as NMI\n",
    "\n",
    "    sequential_ami_scores = []\n",
    "    first_ami_scores = []\n",
    "    sequential_nmi_scores = []\n",
    "    first_nmi_scores = []\n",
    "\n",
    "    for i in range(len(list_of_partition_labels)):\n",
    "\n",
    "        if i == 0:\n",
    "            ami_score = AMI(list_of_partition_labels[i], list_of_partition_labels[i])\n",
    "            sequential_ami_scores.append(ami_score)\n",
    "            first_ami_scores.append(ami_score)\n",
    "\n",
    "            nmi_score = NMI(list_of_partition_labels[i], list_of_partition_labels[i])\n",
    "            sequential_nmi_scores.append(nmi_score)\n",
    "            first_nmi_scores.append(nmi_score)\n",
    "            continue\n",
    "\n",
    "        non_zero_values_list1 = [x for x, y in zip(list_of_partition_labels[i], list_of_partition_labels[i-1]) if x != 0 and y != 0]\n",
    "        non_zero_values_list2 = [y for x, y in zip(list_of_partition_labels[i], list_of_partition_labels[i-1]) if x != 0 and y != 0]\n",
    "\n",
    "        ami_score_sequential = AMI(non_zero_values_list1, non_zero_values_list2)\n",
    "        sequential_ami_scores.append(ami_score_sequential)\n",
    "\n",
    "        nmi_score_sequential = NMI(non_zero_values_list1, non_zero_values_list2)\n",
    "        sequential_nmi_scores.append(nmi_score_sequential)\n",
    "\n",
    "        non_zero_values_list3 = [x for x, y in zip(list_of_partition_labels[i], list_of_partition_labels[0]) if x != 0 and y != 0]\n",
    "        non_zero_values_list4 = [y for x, y in zip(list_of_partition_labels[i], list_of_partition_labels[0]) if x != 0 and y != 0]\n",
    "\n",
    "        first_ami_score = AMI(non_zero_values_list3, non_zero_values_list4)\n",
    "        first_ami_scores.append(first_ami_score)\n",
    "\n",
    "        first_nmi_score = NMI(non_zero_values_list3, non_zero_values_list4)\n",
    "        first_nmi_scores.append(first_nmi_score)\n",
    "\n",
    "    df5 = pd.DataFrame({f'sequential_ami_scores_{string}':sequential_ami_scores, f'sequential_nmi_scores_{string}':sequential_nmi_scores, f'first_ami_scores_{string}':first_ami_scores, f'first_nmi_scores_{string}':first_nmi_scores}, index=window)\n",
    "    df5.plot(figsize=(12, 6), style={f'sequential_ami_scores_{string}': '-', f'sequential_nmi_scores_{string}': '--', f'first_ami_scores_{string}': ':', f'first_nmi_scores_{string}': '-.'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('Similarity value')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title5 = \"\"\n",
    "    if string == 'total':  \n",
    "        title5 = 'Similarity index of communities of citibike trips'\n",
    "#         plt.title('Similarity index of communities of citibike trips')\n",
    "    else:\n",
    "        title5 = f'Similarity index of communities of citibike trips made by {string}s'\n",
    "#         plt.title(f'Similarity index of communities of citibike trips made by {string}s')\n",
    "    plt.title(title5)\n",
    "    plt.grid()\n",
    "    plt.legend(['Sequential AMI score', 'Sequential NMI score', 'Partial AMI score', 'Partial NMI score'])\n",
    "    temp5 = title5+\".png\"\n",
    "    full_path5 = os.path.join(base_eda_dir,temp5)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path5, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# =============================================================================================================================\n",
    "    # Degree Sequence\n",
    "    \n",
    "    high_in_degree_percent = []\n",
    "    high_out_degree_percent = []\n",
    "    low_in_degree_percent = []\n",
    "    low_out_degree_percent = []\n",
    "\n",
    "    for G in graphs:\n",
    "\n",
    "        in_degree_sequence = sorted([d for n, d in G.in_degree()])\n",
    "\n",
    "        high_in_nums = [d for d in in_degree_sequence if d >= 0.6*(G.number_of_nodes())]\n",
    "        low_in_nums = [d for d in in_degree_sequence if d <= 0.1*(G.number_of_nodes())]\n",
    "\n",
    "        out_degree_sequence = sorted([d for n, d in G.out_degree()])\n",
    "\n",
    "        high_out_nums = [d for d in out_degree_sequence if d >= 0.6*(G.number_of_nodes())]\n",
    "        low_out_nums = [d for d in out_degree_sequence if d <= 0.1*(G.number_of_nodes())]\n",
    "\n",
    "        high_in_degree_percent.append(len(high_in_nums)*100/(G.number_of_nodes()) if len(G)>0 else 0)\n",
    "        high_out_degree_percent.append(len(high_out_nums)*100/(G.number_of_nodes()) if len(G)>0 else 0)\n",
    "        low_in_degree_percent.append(len(low_in_nums)*100/(G.number_of_nodes()) if len(G)>0 else 0)\n",
    "        low_out_degree_percent.append(len(low_out_nums)*100/(G.number_of_nodes()) if len(G)>0 else 0)\n",
    "\n",
    "    df6 = pd.DataFrame({f'high_in_degree_percent_{string}':high_in_degree_percent, f'high_out_degree_percent_{string}':high_out_degree_percent, f'low_in_degree_percent_{string}':low_in_degree_percent, f'low_out_degree_percent_{string}':low_out_degree_percent}, index=window)\n",
    "    df6.plot(figsize=(12, 6), style={f'high_in_degree_percent_{string}': '-', f'high_out_degree_percent_{string}': '--', f'low_in_degree_percent_{string}': ':', f'low_out_degree_percent_{string}': '-.'})\n",
    "    plt.xlabel('window')\n",
    "    plt.ylabel('Percentage(%)')\n",
    "    plt.xticks(fontsize=7)\n",
    "    title6 = \"\"\n",
    "    if string == 'total':  \n",
    "        title6 = 'Number of highly busy and least busy citibike stations'\n",
    "#         plt.title('Number of highly busy and least busy citibike stations')\n",
    "    else:\n",
    "        title6 = f'Number of highly busy and least busy citibike stations used by {string}s'\n",
    "#         plt.title(f'Number of highly busy and least busy citibike stations used by {string}s')\n",
    "    plt.title(title6)\n",
    "    plt.grid()\n",
    "    plt.legend(['High incoming traffic(>= 60% of nodes)', 'High outgoing traffic(>= 60% of nodes)', 'Low incoming traffic(<= 10% of nodes)', 'Low outgoing traffic(<= 10% of nodes)'])\n",
    "    temp6 = title6+\".png\"\n",
    "    full_path6 = os.path.join(base_eda_dir,temp6)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path6, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# ==============================================================================================================================\n",
    "    # Centrality measures\n",
    "    \n",
    "    in_degree_centrality = [0]*263\n",
    "    out_degree_centrality = [0]*263\n",
    "    betweenness_centrality = [0]*263 #without weights\n",
    "    eigenvector_centrality = [0]*263 #with weights\n",
    "\n",
    "    for G in graphs:\n",
    "        \n",
    "        if len(G) == 0: continue\n",
    "\n",
    "        in_degrees = nx.in_degree_centrality(G)\n",
    "        out_degrees = nx.out_degree_centrality(G)\n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        eigenvector = nx.eigenvector_centrality(G,max_iter=1000,tol=1e-4, weight='weight')\n",
    "\n",
    "        max_in = max(in_degrees.values())\n",
    "        max_out = max(out_degrees.values())\n",
    "        max_betweenness = max(betweenness.values())\n",
    "        max_eigenvector = max(eigenvector.values())\n",
    "\n",
    "        max_ins = [key for key, value in in_degrees.items() if value == max_in]\n",
    "        max_outs = [key for key, value in out_degrees.items() if value == max_out]\n",
    "        max_betweenness_s = [key for key, value in betweenness.items() if value == max_betweenness]\n",
    "        max_eigenvectors = [key for key, value in eigenvector.items() if value == max_eigenvector]\n",
    "\n",
    "        in_degree_centrality = [val+1 if idx in max_ins else val for idx, val in enumerate(in_degree_centrality)]\n",
    "        out_degree_centrality = [val+1 if idx in max_outs else val for idx, val in enumerate(out_degree_centrality)]\n",
    "        betweenness_centrality = [val+1 if idx in max_betweenness_s else val for idx, val in enumerate(betweenness_centrality)]\n",
    "        eigenvector_centrality = [val+1 if idx in max_eigenvectors else val for idx, val in enumerate(eigenvector_centrality)]\n",
    "\n",
    "    in_degree_centrality = {index: value for index, value in enumerate(in_degree_centrality)}\n",
    "    out_degree_centrality = {index: value for index, value in enumerate(out_degree_centrality)}\n",
    "    betweenness_centrality = {index: value for index, value in enumerate(betweenness_centrality)}\n",
    "    eigenvector_centrality = {index: value for index, value in enumerate(eigenvector_centrality)}\n",
    "\n",
    "    sorted_in_degree_centrality = dict(sorted(in_degree_centrality.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_out_degree_centrality = dict(sorted(out_degree_centrality.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_betweenness_centrality = dict(sorted(betweenness_centrality.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_eigenvector_centrality = dict(sorted(eigenvector_centrality.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    top_10_in_degree = dict(list(sorted_in_degree_centrality.items())[:10])\n",
    "    top_10_out_degree = dict(list(sorted_out_degree_centrality.items())[:10])\n",
    "    top_10_betweenness = dict(list(sorted_betweenness_centrality.items())[:10])\n",
    "    top_10_eigenvector = dict(list(sorted_eigenvector_centrality.items())[:10])\n",
    "    \n",
    "    zones = pd.read_csv('Data/taxi_zone_lookup.csv')\n",
    "    zones = zones.drop(['Borough', 'service_zone'], axis=1)\n",
    "# =====================================================================================================================\n",
    "    # in degree centrality\n",
    "    top_non_zero_in_degree = {k: v for k, v in top_10_in_degree.items() if v != 0}\n",
    "    nodes_strings_in_degree = [zones.loc[zones['LocationID'] == node+1, 'Zone'].iloc[0] for node in top_non_zero_in_degree.keys()]\n",
    "    frequencies_in_degree = list(top_non_zero_in_degree.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(nodes_strings_in_degree, frequencies_in_degree)  # Flip x and y axis\n",
    "    plt.ylabel('Zone')  # Set y-axis label\n",
    "    plt.xlabel('Frequency')  # Set x-axis label\n",
    "    title7 = \"\"\n",
    "    if string == 'total':  \n",
    "        title7 = 'Most central zones based on in degree centrality'\n",
    "#         plt.title('Most central zones based on in degree centrality')\n",
    "    else:\n",
    "        title7 = f'Most central zones used by {string}s based on in degree centrality'\n",
    "#         plt.title(f'Most central zones used by {string}s based on in degree centrality')\n",
    "    plt.title(title7)\n",
    "    temp7 = title7+\".png\"\n",
    "    full_path7 = os.path.join(base_eda_dir,temp7)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path7, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# =========================================================================================================================\n",
    "    # out degree centrality\n",
    "    top_non_zero_out_degree = {k: v for k, v in top_10_out_degree.items() if v != 0}\n",
    "    nodes_strings_out_degree = [zones.loc[zones['LocationID'] == node+1, 'Zone'].iloc[0] for node in top_non_zero_out_degree.keys()]\n",
    "    frequencies_out_degree = list(top_non_zero_out_degree.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(nodes_strings_out_degree, frequencies_out_degree)  # Flip x and y axis\n",
    "    plt.ylabel('Zone')  # Set y-axis label\n",
    "    plt.xlabel('Frequency')  # Set x-axis label\n",
    "    title8 = \"\"\n",
    "    if string == 'total':  \n",
    "        title8 = 'Most central zones based on out degree centrality'\n",
    "#         plt.title('Most central zones based on out degree centrality')\n",
    "    else:\n",
    "        title8 = f'Most central zones used by {string}s based on out degree centrality'\n",
    "#         plt.title(f'Most central zones used by {string}s based on out degree centrality')\n",
    "    plt.title(title8)\n",
    "    temp8 = title8+\".png\"\n",
    "    full_path8 = os.path.join(base_eda_dir,temp8)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path8, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# =====================================================================================================================\n",
    "    # betweenness centrality\n",
    "    top_non_zero_betweenness = {k: v for k, v in top_10_betweenness.items() if v != 0}\n",
    "    nodes_strings_betweenness = [zones.loc[zones['LocationID'] == node+1, 'Zone'].iloc[0] for node in top_non_zero_betweenness.keys()]\n",
    "    frequencies_betweenness = list(top_non_zero_betweenness.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(nodes_strings_betweenness, frequencies_betweenness)  # Flip x and y axis\n",
    "    plt.ylabel('Zone')  # Set y-axis label\n",
    "    plt.xlabel('Frequency')  # Set x-axis label\n",
    "    title9 = \"\"\n",
    "    if string == 'total':  \n",
    "        title9 = 'Most central zones based on betweenness centrality'\n",
    "#         plt.title('Most central zones based on betweenness centrality')\n",
    "    else:\n",
    "        title9 = f'Most central zones used by {string}s based on betweenness centrality'\n",
    "#         plt.title(f'Most central zones used by {string}s based on betweenness centrality')\n",
    "    plt.title(title9)\n",
    "    temp9 = title9+\".png\"\n",
    "    full_path9 = os.path.join(base_eda_dir,temp9)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path9, bbox_inches='tight')\n",
    "    plt.show()\n",
    "# =========================================================================================================================\n",
    "    # eigenvector centrality\n",
    "    top_non_zero_eigenvector = {k: v for k, v in top_10_eigenvector.items() if v != 0}\n",
    "    nodes_strings_eigenvector = [zones.loc[zones['LocationID'] == node+1, 'Zone'].iloc[0] for node in top_non_zero_eigenvector.keys()]\n",
    "    frequencies_eigenvector = list(top_non_zero_eigenvector.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(nodes_strings_eigenvector, frequencies_eigenvector)  # Flip x and y axis\n",
    "    plt.ylabel('Zone')  # Set y-axis label\n",
    "    plt.xlabel('Frequency')  # Set x-axis label\n",
    "    title10 = \"\"\n",
    "    if string == 'total':  \n",
    "        title10 = 'Most central zones based on eigenvector centrality'\n",
    "#         plt.title('Most central zones based on eigenvector centrality')\n",
    "    else:\n",
    "        title10 = f'Most central zones used by {string}s based on eigenvector centrality'\n",
    "#         plt.title(f'Most central zones used by {string}s based on eigenvector centrality')\n",
    "    plt.title(title10)\n",
    "    temp10 = title10+\".png\"\n",
    "    full_path10 = os.path.join(base_eda_dir,temp10)\n",
    "    if not os.path.exists(base_eda_dir):\n",
    "        os.makedirs(base_eda_dir)\n",
    "    plt.savefig(full_path10, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weeklyAggregatedCitibike.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_graphs_total, monthly_graphs_customers, monthly_graphs_subscribers, days = sliding_window_graph_creation(df, 28, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c007e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_EDA(monthly_graphs_total, days, 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba898c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_EDA(monthly_graphs_customers, days, 'customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc63a22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph_EDA(monthly_graphs_subscribers, days, 'subscriber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa069154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
